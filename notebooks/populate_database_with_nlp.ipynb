{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prospective-bhutan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
    "from py2neo.bulk import merge_nodes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-printing",
   "metadata": {},
   "source": [
    "# Configure spacy\n",
    "\n",
    "Prior to actually using spacy, we need to load in some models.  The basic model is their small core library, taken from the web: `en_core_web_sm`, which provides good, basic functionality with a small download size (< 20 MB).  However, one drawback of this basic model is that it doesn't have full word vectors.  Instead, it comes with context-sensitive tensors.  You can still do things like text similarity with it, but if you want to use spacy to create good word vectors, you should use a larger model such as `en_core_web_md` or`en_core_web_lg` since the small models are not known for accuracy.  You can also use a variety of third-party models, but that is beyond the scope of this workshop.  Again, choose the model that works best with your setup.\n",
    "\n",
    "In general, to load the models we use the following command:\n",
    "\n",
    "`python3 -m spacy download en_core_web_md`\n",
    "\n",
    "This has already been completed in the container, but if you want to add other models you can do this either as a cell in this notebook or via the CLI.\n",
    "\n",
    "## API key for Google Knowledge Graph\n",
    "\n",
    "See below for instructions on how to create this key.  When you have the key, save it to a file called `.api_key` in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collectible-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'merge_noun_chunks']\n"
     ]
    }
   ],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "VERBS = ['ROOT', 'advcl']\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
    "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
    "\n",
    "api_key = open('.api_key').read()\n",
    "\n",
    "non_nc = spacy.load('en_core_web_md')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('merge_noun_chunks')\n",
    "\n",
    "print(non_nc.pipe_names)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-caribbean",
   "metadata": {},
   "source": [
    "# Query Google Knowledge Graph\n",
    "\n",
    "To query the Google Knowledge Graph you will require an API key, which permits you to have 100,000 read calls per day per project for free.  That will be more than sufficient for this workshop.  To obtain your key, follow [these instructions](https://developers.google.com/knowledge-graph/how-tos/authorizing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "experienced-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
    "    \n",
    "    text_ls = []\n",
    "    node_label_ls = []\n",
    "    url_ls = []\n",
    "    \n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': limit,\n",
    "        'indent': indent,\n",
    "        'key': api_key,\n",
    "    }   \n",
    "    \n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    \n",
    "    if return_lists:\n",
    "        for element in response['itemListElement']:\n",
    "\n",
    "            try:\n",
    "                node_label_ls.append(element['result']['@type'])\n",
    "            except:\n",
    "                node_label_ls.append('')\n",
    "\n",
    "            try:\n",
    "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
    "            except:\n",
    "                text_ls.append('')\n",
    "                \n",
    "            try:\n",
    "                url_ls.append(element['result']['detailedDescription']['url'])\n",
    "            except:\n",
    "                url_ls.append('')\n",
    "                \n",
    "        return text_ls, node_label_ls, url_ls\n",
    "    \n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-norman",
   "metadata": {},
   "source": [
    "# NLP Functions\n",
    "\n",
    "There are a variety of functions below that will use both spacy and regex to clean and prepare our data prior to loading it into the graph database.  The main goal is to create subject-verb-object (SVO) triples.  But before we can do that, we have to do some general cleaning of the data.  The below functions perform this for us in the following order:\n",
    "\n",
    "1. Use regex to remove control characters (`remove_special_characters`)\n",
    "2. Create spacy doc of (1) (`create_svo_lists`)\n",
    "3. Get list of all subjects, verbs, and objects (`create_svo_lists`)\n",
    "4. For each object, find the closest verb (`create_svo_triples`)\n",
    "5. Remove all stop words and punctuation (`remove_stop_words_and_punct`)\n",
    "6. Assemble SVO tuples (`create_svo_triples`)\n",
    "7. Remove duplicate tuples (`remove_duplicates`)\n",
    "8. Remove tuples that have dates in them (`remove_dates`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "absent-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    clean_text = regex.sub(\" \", text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_stop_words_and_punct(text, print_text=False):\n",
    "    \n",
    "    result_ls = []\n",
    "    rsw_doc = non_nc(text)\n",
    "    \n",
    "    for token in rsw_doc:\n",
    "        if print_text:\n",
    "            print(token, token.is_stop)\n",
    "            print('--------------')\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            result_ls.append(str(token))\n",
    "    \n",
    "    result_str = ' '.join(result_ls)\n",
    "\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def create_svo_lists(doc, print_lists=False):\n",
    "    \n",
    "    subject_ls = []\n",
    "    verb_ls = []\n",
    "    object_ls = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ in SUBJECTS:\n",
    "            subject_ls.append((token.lower_, token.idx))\n",
    "        elif token.dep_ in VERBS:\n",
    "            verb_ls.append((token.lemma_, token.idx))\n",
    "        elif token.dep_ in OBJECTS:\n",
    "            object_ls.append((token.lower_, token.idx))\n",
    "\n",
    "    if print_lists:\n",
    "        print('SUBJECTS: ', subject_ls)\n",
    "        print('VERBS: ', verb_ls)\n",
    "        print('OBJECTS: ', object_ls)\n",
    "    \n",
    "    return subject_ls, verb_ls, object_ls\n",
    "\n",
    "\n",
    "def remove_duplicates(tup, tup_posn):\n",
    "    \n",
    "    check_val = set()\n",
    "    result = []\n",
    "    \n",
    "    for i in tup:\n",
    "        if i[tup_posn] not in check_val:\n",
    "            result.append(i)\n",
    "            check_val.add(i[tup_posn])\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_dates(tup_ls):\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    for entry in tup_ls:\n",
    "        if not entry[2].isdigit():\n",
    "            clean_tup_ls.append(entry)\n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_svo_triples(text):\n",
    "    \n",
    "    clean_text = remove_special_characters(text)\n",
    "    doc = nlp(clean_text)\n",
    "    subject_ls, verb_ls, object_ls = create_svo_lists(doc)\n",
    "    \n",
    "    graph_tup_ls = []\n",
    "    dedup_tup_ls = []\n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for subj in subject_ls: \n",
    "        for obj in object_ls:\n",
    "            \n",
    "            dist_ls = []\n",
    "            \n",
    "            for v in verb_ls:\n",
    "                \n",
    "                # Assemble a list of distances between each object and each verb\n",
    "                dist_ls.append(abs(obj[1] - v[1]))\n",
    "                \n",
    "            # Get the index of the verb with the smallest distance to the object \n",
    "            # and return that verb\n",
    "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
    "            \n",
    "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
    "            # later down in the process to allow for proper sentence recognition.\n",
    "\n",
    "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
    "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
    "            \n",
    "            # Add entries to the graph iff neither subject nor object is blank\n",
    "            if no_sw_subj and no_sw_obj:\n",
    "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
    "                graph_tup_ls.append(tup)\n",
    "        \n",
    "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
    "    \n",
    "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
    "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
    "    \n",
    "    return clean_tup_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-vacation",
   "metadata": {},
   "source": [
    "# Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appointed-variation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Hussein Obama II ( (listen) bə-RAHK hoo-SAYN oh-BAH-mə; born August 4, 1961) is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, Obama was the first African-American  president of the United States. He previously served as a U.S. senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004.\\nObama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School, where he was the first black person to be president of the Harvard Law Review. After graduating, he became a civil rights attorney and an academic, teaching constitutional law at the University of Chicago Law School from 1992 to 2004. Turning to elective politics, he represented the 13th district in the Illinois Senate from 1997 until 2004, when he ran for the U.S. Senate. Obama received national attention in 2004 with his March Senate primary win, his well-received July Democratic National Convention keynote address, and his landslide November election to the Senate. In 2008, he was nominated by the Democratic Party for president a year after beginning his campaign, and after a close primary campaign against Hillary Clinton. Obama was elected over Republican nominee John McCain in the general election and was inaugurated alongside his running mate, Joe Biden, on January 20, 2009. Nine months later, he was named the 2009 Nobel Peace Prize laureate.\\nObama signed many landmark bills into law during his first two years in office. The main reforms that were passed include the Affordable Care Act (commonly referred to as ACA or \"Obamacare\"), although without a public health insurance option, the Dodd–Frank Wall Street Reform and Consumer Protection Act, and the Don\\'t Ask, Don\\'t Tell Repeal Act of 2010. The American Recovery and Reinvestment Act of 2009 and Tax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010 served as economic stimuli amidst the Great Recession. After a lengthy debate over the national debt limit, he signed the Budget Control and the American Taxpayer Relief Acts. In foreign policy, he increased U.S. troop levels in Afghanistan, reduced nuclear weapons with the United States–Russia New START treaty, and ended military involvement in the Iraq War. He ordered military involvement in Libya for the implementation of the UN Security Council Resolution 1973, contributing to the overthrow of Muammar Gaddafi. He also ordered the military operation that resulted in the killing of Osama bin Laden.\\nAfter winning re-election by defeating Republican opponent Mitt Romney, Obama was sworn in for a second term in 2013. During this term, he promoted inclusion for LGBT Americans. His administration filed briefs that urged the Supreme Court to strike down same-sex marriage bans as unconstitutional (United States v. Windsor and Obergefell v. Hodges); same-sex marriage was legalized nationwide in 2015 after the Court ruled so in Obergefell. He advocated for gun control in response to the Sandy Hook Elementary School shooting, indicating support for a ban on assault weapons, and issued wide-ranging executive actions concerning global warming and immigration. In foreign policy, he ordered military intervention in Iraq in response to gains made by ISIL after the 2011 withdrawal from Iraq, continued the process of ending U.S. combat operations in Afghanistan in 2016, promoted discussions that led to the 2015 Paris Agreement on global climate change, initiated sanctions against Russia following the invasion in Ukraine and again after interference in the 2016 U.S. elections, brokered the JCPOA nuclear deal with Iran, and normalized U.S. relations with Cuba. Obama nominated three justices to the Supreme Court: Sonia Sotomayor and Elena Kagan were confirmed as justices, while Merrick Garland faced partisan obstruction from the Republican-led Senate led by Mitch McConnell, which never held hearings or a vote on the nomination. Obama left office in January 2017 and continues to reside in Washington, D.C. During Obama\\'s terms in office, the United States\\' reputation abroad, as well as the American economy, significantly improved. Obama\\'s presidency has generally been regarded favorably, and evaluations of his presidency among historians, political scientists, and the general public frequently place him among the upper tier of American presidents.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = wikipedia.summary('barack obama')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-american",
   "metadata": {},
   "source": [
    "# Preprocess the data (including NLP)\n",
    "\n",
    "We have obtained our text from Wikipedia.  Now we will prepare it for the graph.  This will be done in the following steps:\n",
    "\n",
    "1. We create the SVO triples for the initial test (`create_svo_triples`)\n",
    "2. Get some node properties for each object such as the different possible label types, a brief description, and the URL provided by the Google Knowledge Graph (`get_obj_properties`)\n",
    "3. Add a new layer to the graph that is the SVOs associated with each object.  This is done to build out the graph a bit and make it more rich. (`add_layer`)\n",
    "4. Combine the initial tuple list from (1) with the expanded list from (3)\n",
    "5. Eliminate tuples where the subject is equal to the object (`subj_equals_obj`)\n",
    "6. Create word embeddings of the object descriptions (`create_word_vectors`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hispanic-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_properties(tup_ls):\n",
    "    \n",
    "    init_obj_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "\n",
    "        try:\n",
    "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
    "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
    "        except:\n",
    "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
    "        \n",
    "        init_obj_tup_ls.append(new_tup)\n",
    "        \n",
    "    return init_obj_tup_ls\n",
    "\n",
    "\n",
    "def add_layer(tup_ls):\n",
    "\n",
    "    svo_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        \n",
    "        if tup[3]:\n",
    "            svo_tup = create_svo_triples(tup[3])\n",
    "            svo_tup_ls.extend(svo_tup)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return get_obj_properties(svo_tup_ls)\n",
    "        \n",
    "\n",
    "def subj_equals_obj(tup_ls):\n",
    "    \n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[0] != tup[2]:\n",
    "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
    "            \n",
    "    return new_tup_ls\n",
    "\n",
    "\n",
    "def check_for_string_labels(tup_ls):\n",
    "    # This is for an edge case where the object does not get fully populated\n",
    "    # resulting in the node labels being assigned to string instead of list.\n",
    "    # This may not be strictly necessary and the lines using it are commnted out\n",
    "    # below.  Run this function if you come across this case.\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        if isinstance(el[2], list):\n",
    "            clean_tup_ls.append(el)\n",
    "            \n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_word_vectors(tup_ls):\n",
    "\n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[3]:\n",
    "            doc = nlp(tup[3])\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
    "        else:\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
    "        new_tup_ls.append(new_tup)\n",
    "        \n",
    "    return new_tup_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-matter",
   "metadata": {},
   "source": [
    "# Let's now run this step by step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "similar-caution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.8 s, sys: 21.3 ms, total: 49.9 s\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "initial_tup_ls = create_svo_triples(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "general-estimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 458 ms, total: 23.8 s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
    "new_layer_ls = add_layer(init_obj_tup_ls)\n",
    "starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
    "edge_ls = subj_equals_obj(starter_edge_ls)\n",
    "#clean_edge_ls = check_for_string_labels(edge_ls)\n",
    "#clean_edge_ls[0:3]\n",
    "clean_edge_ls = edge_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "attended-vitamin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oh bah mə', 'be', 'american politician', '', ['Thing'], ''),\n",
       " ('oh bah mə', 'be', '44th president', [], [], []),\n",
       " ('oh bah mə',\n",
       "  'be',\n",
       "  'united states',\n",
       "  'The United States of America, commonly known as the United States, or America, is a country primarily located in North America. ',\n",
       "  ['AdministrativeArea', 'Thing', 'Country', 'Place'],\n",
       "  'https://en.wikipedia.org/wiki/United_States')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_ls[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tough-economics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.7 s, sys: 3.91 ms, total: 3.7 s\n",
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "edges_word_vec_ls = create_word_vectors(edge_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-football",
   "metadata": {},
   "source": [
    "# Create the node and edge lists to populate the graph with the below helper functions\n",
    "\n",
    "These functions achieve the following:\n",
    "\n",
    "1. Deduping of the node list (`dedup`)\n",
    "2. Under the idea that we might want to use word embeddings, we are pulling them in as a node property.  However, Neo4j doesn't work well with numpy arrays, so we convert that array to a list of floats (`convert_vec_to_ls`).\n",
    "3. Add nodes to the graph with the `py2neo` bulk importer (`add_nodes`)\n",
    "4. Add edges (relationships) to the graph (`add_edges`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "convenient-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(tup_ls):\n",
    "    \n",
    "    visited = set()\n",
    "    output_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if not tup[0] in visited:\n",
    "            visited.add(tup[0])\n",
    "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
    "            \n",
    "    return output_ls\n",
    "\n",
    "\n",
    "def convert_vec_to_ls(tup_ls):\n",
    "    \n",
    "    vec_to_ls_tup = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        vec_ls = [float(v) for v in el[4]]\n",
    "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
    "        vec_to_ls_tup.append(tup)\n",
    "        \n",
    "    return vec_to_ls_tup\n",
    "\n",
    "\n",
    "def add_nodes(tup_ls):   \n",
    "\n",
    "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
    "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
    "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "gorgeous-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges(edge_ls):\n",
    "    \n",
    "    edge_dc = {} \n",
    "    \n",
    "    # Group tuple by verb\n",
    "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
    "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
    "    \n",
    "    for tup in edge_ls: \n",
    "        if tup[1] in edge_dc: \n",
    "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
    "        else: \n",
    "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
    "    \n",
    "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
    "        \n",
    "        tx = graph.begin()\n",
    "        \n",
    "        for el in tup_ls:\n",
    "            source_node = nodes_matcher.match(name=el[0]).first()\n",
    "            target_node = nodes_matcher.match(name=el[2]).first()\n",
    "            if not source_node:\n",
    "                source_node = Node('Node', name=el[0])\n",
    "                tx.create(source_node)\n",
    "            if not target_node:\n",
    "                try:\n",
    "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
    "                    tx.create(target_node)\n",
    "                except:\n",
    "                    continue\n",
    "            try:\n",
    "                rel = Relationship(source_node, edge_labels, target_node)\n",
    "            except:\n",
    "                continue\n",
    "            tx.create(rel)\n",
    "        tx.commit()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-birthday",
   "metadata": {},
   "source": [
    "# Creating some lists of tuples representing the node and edge lists\n",
    "\n",
    "For our node list, we begin with the query subject (Barack Obama), which is in `edge_ls[0][0]` and put this in the variable `orig_node_tup_ls`.  We assume a node label of `Subject` and no description or word vector.  We then add all of the objects associated with the `edges_word_vec_ls` (`obj_node_tup_ls`) and combine that with the previous variable to create `full_node_tup_ls`, which we then dedupe into `dedup_node_tup_ls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "incident-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_node_tup_ls = [(edge_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
    "obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
    "full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
    "dedup_node_tup_ls = dedup(full_node_tup_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "opponent-production",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 523)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_node_tup_ls), len(dedup_node_tup_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-senator",
   "metadata": {},
   "source": [
    "# Create the node list that will be used to populate the graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tribal-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-briefing",
   "metadata": {},
   "source": [
    "# Populate the graph\n",
    "\n",
    "Here we establish a connection to the database, which is running on the internal Docker network called `neo4j`.  We provide it the user name and password and also create a class for matching nodes (used when we establish the edges in the graph).  Finally, we add the nodes and edges to populate the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bearing-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://neo4j:7687\", name=\"neo4j\", password=\"1234\")\n",
    "nodes_matcher = NodeMatcher(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "automotive-latest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in graph:  523\n"
     ]
    }
   ],
   "source": [
    "add_nodes(node_tup_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "soviet-newman",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:08<00:00,  7.65it/s]\n"
     ]
    }
   ],
   "source": [
    "add_edges(edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-worker",
   "metadata": {},
   "source": [
    "# But we have some things to explore in the DB before we think we are done with populating the graph...  \n",
    "\n",
    "Note: I am going to collapse a lot of the tuple list creation into a few functions to make it \"easier\" on the eyes.\n",
    "\n",
    "We are going to delete all entries in the database and start it fresh.  The reason for this is that the nodes in there already have labels.  We are going to run our Cypher query to create the new labels, but it would be complicated to do that selectively based just on the new nodes added to the graph after our original EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "impaired-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_tuple_creation(text):\n",
    "    \n",
    "    initial_tup_ls = create_svo_triples(text)\n",
    "    init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
    "    new_layer_ls = add_layer(init_obj_tup_ls)\n",
    "    starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
    "    edge_ls = subj_equals_obj(starter_edge_ls)\n",
    "    edges_word_vec_ls = create_word_vectors(edge_ls)\n",
    "    \n",
    "    return edges_word_vec_ls\n",
    "\n",
    "\n",
    "def node_tuple_creation(edges_word_vec_ls):\n",
    "    \n",
    "    orig_node_tup_ls = [(edges_word_vec_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
    "    obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
    "    full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
    "    cleaned_node_tup_ls = check_for_string_labels(full_node_tup_ls)\n",
    "    dedup_node_tup_ls = dedup(cleaned_node_tup_ls)\n",
    "    node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)\n",
    "    \n",
    "    return node_tup_ls    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hazardous-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 608 ms, total: 1min 30s\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "barack_text = wikipedia.summary('barack obama')\n",
    "barack_edges_word_vec_ls = edge_tuple_creation(barack_text)\n",
    "barack_node_tup_ls = node_tuple_creation(barack_edges_word_vec_ls)\n",
    "\n",
    "michelle_text = wikipedia.summary('michelle obama')\n",
    "michelle_edges_word_vec_ls = edge_tuple_creation(michelle_text)\n",
    "michelle_node_tup_ls = node_tuple_creation(michelle_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "conscious-processing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670 636\n"
     ]
    }
   ],
   "source": [
    "full_node_ls = barack_node_tup_ls + michelle_node_tup_ls\n",
    "full_edge_ls = barack_edges_word_vec_ls + michelle_edges_word_vec_ls\n",
    "full_dedup_node_tup_ls = dedup(full_node_ls)\n",
    "print(len(full_node_ls), len(full_dedup_node_tup_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "organized-democrat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in graph:  636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:07<00:00,  9.93it/s]\n"
     ]
    }
   ],
   "source": [
    "add_nodes(full_dedup_node_tup_ls)\n",
    "add_edges(full_edge_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-perspective",
   "metadata": {},
   "source": [
    "# Entity disambiguation\n",
    "\n",
    "In this notebook we will calculate the cosine similarity of the word vectors of target nodes to try and determine the likelihood of two nodes being the same entity.  Note that you could do this directly with spacy (which defaults to cosine similarity) prior to anything above through something like:\n",
    "\n",
    "```\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "doc1.similarity(doc2)\n",
    "```\n",
    "\n",
    "However, since we have already calculated the word vectors for each node description above, we will just use the cosine similarity built into `scikit-learn`. \n",
    "\n",
    "Note that we are creating now a complete node list since the nodes we are comparing might be either in the Barack or the Michelle Obama node lists..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "short-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vec_similarity(node1, node2, node_ls):\n",
    "    \n",
    "    node1_vec = [tup[4] for tup in node_ls if tup[0] == node1]\n",
    "    node2_vec = [tup[4] for tup in node_ls if tup[0] == node2]\n",
    "    \n",
    "    return cosine_similarity(node1_vec, node2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "governing-effects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "cs = get_word_vec_similarity('barack obama', 'president barack obama', full_dedup_node_tup_ls)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-minneapolis",
   "metadata": {},
   "source": [
    "## ...OK, that one was cheating...\n",
    "\n",
    "...because the Google Knowledge Graph knew that these were the same thing.  The NER returned these as being different nodes, but Google knew better and gave the exact description for each.  This will not necessarily be the case in more complicated knowledge graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "veterinary-quantity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89986608]]\n"
     ]
    }
   ],
   "source": [
    "cs = get_word_vec_similarity('barack obama', 'mitch mcconnell', full_dedup_node_tup_ls)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "governing-bible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94099334]]\n"
     ]
    }
   ],
   "source": [
    "cs = get_word_vec_similarity('barack obama', 'hillary clinton', full_dedup_node_tup_ls)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "elementary-invalid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89470368]]\n"
     ]
    }
   ],
   "source": [
    "cs = get_word_vec_similarity('barack obama', 'harvard law school', full_dedup_node_tup_ls)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hidden-proposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87748002]]\n"
     ]
    }
   ],
   "source": [
    "cs = get_word_vec_similarity('barack obama', 'oman', full_dedup_node_tup_ls)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-gross",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
